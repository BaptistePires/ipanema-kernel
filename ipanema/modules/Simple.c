/*
  Automatically generated from an Ipanema specification by the Ipanema compiler
  Copyright (C) 2016 INRIA, LIG, EPFL, I3S
  --- Do not edit this file ---
*/

/* checksum = 13904 */
#define LINUX

#include <linux/delay.h>
#include <linux/ipanema.h>
#include <linux/ipanema_rbtree.h>
#include <linux/ktime.h>
#include <linux/lockdep.h>
#include <linux/module.h>
#include <linux/proc_fs.h>
#include <linux/sched.h>
#include <linux/seq_file.h>
#include <linux/slab.h>
#include <linux/sort.h>
#include <linux/threads.h>

#define ipanema_assert(x) do{if(!(x)) panic("Error in " #x "\n");} while(0)
#define time_to_ticks(x) ktime_to_ns(x)
#define ticks_to_time(x) ns_to_ktime(x)

static char *name = "Simple";
static struct ipanema_module *module;

#define	 CURRENT_0_STATE  1 << 0 /* File "compiler/compiler/compile_misc.ml", line 67, characters 43-50 */
#define	 READY_STATE  1 << 1 /* File "compiler/compiler/compile_misc.ml", line 64, characters 43-50 */
#define	 BLOCKED_STATE	1 << 2 /* File "compiler/compiler/compile_misc.ml", line 64, characters 43-50 */
#define	 TERMINATED_STATE  1 << 3 /* File "compiler/compiler/compile_misc.ml", line 64, characters 43-50 */
#define	 READY_TICK_STATE  1 << 4 /* File "compiler/compiler/compile_misc.ml", line 53, characters 68-75 */
#define	 MIGRATING_STATE  1 << 5 /* File "compiler/compiler/compile_misc.ml", line 54, characters 71-78 */
#define	 ACTIVE_CORES_STATE  1 << 0 /* File "compiler/compiler/compile_misc.ml", line 82, characters 42-49 */
#define	 INACTIVE_CORES_STATE  1 << 1

#define get_policy_current(cpu)    (ipanema_state(cpu).current_0)
#define get_policy_rq(cpu, name)   (ipanema_state(cpu).name)
#define get_policy_core(cpu)       (per_cpu(core, (cpu)))

struct Simple_ipa_process;
struct Simple_ipa_core;
struct Simple_ipa_sched_domain;
struct Simple_ipa_sched_group;

/* definition of protocol states */
struct state_info {
	struct Simple_ipa_process *current_0;
	struct ipanema_rq ready;
};


// At least a READY queue is often shared.
// Optimization: use DEFINE_PER_CPU_ALIGNED(type, name) otherwise.
// See include/linux/percpu-defs.h for more information.
DEFINE_PER_CPU_SHARED_ALIGNED(struct state_info, state_info);

/* definition of core's states */
struct core_state_info {
	cpumask_var_t active_cores;
};


static struct core_state_info cstate_info;

struct Simple_ipa_process {
	/* process attributes
	 *  specified by the scheduling policy
	 *  in the process = {...} declaration
	 */
	int state; // Internal
	struct ipanema_rq *rq; // Internal
	struct rb_node node; // Internal
	struct task_struct * task; // Internal
	int quanta;
	int load;
	struct list_head list;
};

struct Simple_ipa_core {
	/* core attributes
	 *  specified by the scheduling policy
	 *  in the core = {...} declaration
	 */
	enum ipanema_core_state state; // Internal
	int ___sched_domains_idx; // Internal
	int id; // System
	struct Simple_ipa_sched_domain * sd;
	int cload;
};

struct Simple_ipa_sched_group {
	/* group attributes
	 *  specified by the scheduling policy
	 *  in the group = {...} declaration
	 */
	cpumask_var_t cores;
	int capacity;
};

struct Simple_ipa_sched_domain {
	/* domain attributes
	 *  specified by the scheduling policy
	 *  in the domain = {...} declaration
	 */
	int ___sched_group_idx; // Internal
	cpumask_var_t cores; // Internal
	int flags; // Internal
	struct Simple_ipa_sched_group * groups;
	ktime_t next_balance;
	unsigned int idle_count, periodic_count;
};

DEFINE_PER_CPU(struct Simple_ipa_core, core);

static int ipanema_Simple_order_process(struct ipanema_policy *policy,
					struct task_struct *a,
					struct task_struct *b)
{
	struct Simple_ipa_process *pa = policy_metadata(a);
	struct Simple_ipa_process *pb = policy_metadata(b);

	return pa->quanta - pb->quanta;
}

static int get_class(int state)
{
	switch(state) {
		case CURRENT_0_STATE: return IPANEMA_RUNNING;
		case READY_STATE: return IPANEMA_READY;
		case BLOCKED_STATE: return IPANEMA_BLOCKED;
		case TERMINATED_STATE: return IPANEMA_TERMINATED;
		case READY_TICK_STATE: return IPANEMA_READY_TICK;
		case MIGRATING_STATE: return IPANEMA_MIGRATING;
		default: return -1;
	}
}

static void ipa_change_proc(struct Simple_ipa_process *proc,
			    struct Simple_ipa_process **dst,
			    int state)
{
	*dst = proc;
	proc->state = state;
	proc->rq = NULL;
	change_state(proc->task, get_class(state), task_cpu(proc->task), NULL);
}

static void ipa_change_queue(struct Simple_ipa_process *proc,
			     struct ipanema_rq *rq, int state)
{
	if (proc->state == CURRENT_0_STATE)
		ipanema_state(task_cpu(proc->task)).current_0 = NULL;
	proc->state = state;
	proc->rq = rq;
	change_state(proc->task, get_class(state), task_cpu(proc->task), rq);
}


static void ipa_change_queue_and_core(struct Simple_ipa_process *proc,
				      struct ipanema_rq *rq, int state,
				      struct Simple_ipa_core *core)
{
	if (proc->state == CURRENT_0_STATE)
		ipanema_state(task_cpu(proc->task)).current_0 = NULL;
	proc->state = state;
	proc->rq = rq;
	change_state(proc->task, get_class(state), core->id, rq);
}

static void set_active_core(struct Simple_ipa_core *core, cpumask_var_t cores)
{
	core->state = IPANEMA_ACTIVE_CORE;
	cpumask_set_cpu(core->id, cores);
}

static void set_inactive_core(struct Simple_ipa_core *core, cpumask_var_t cores)
{
	core->state = IPANEMA_IDLE_CORE;
	cpumask_clear_cpu(core->id, cores);
}

static enum ipanema_core_state
ipanema_Simple_get_core_state(struct ipanema_policy *policy,
			      struct core_event *e)
{
	return ipanema_core(e->target).state;
}

static int ipanema_Simple_new_prepare(struct ipanema_policy *policy,
				      struct process_event *e)
{
	struct Simple_ipa_process *tgt;
	struct task_struct *p = NULL;
	int cpu, dst;
	int cpu_nr, dst_nr;

	p = e->target;
	tgt = kzalloc(sizeof(struct Simple_ipa_process), GFP_ATOMIC);
	if (!tgt)
		return -1;
	policy_metadata(p) = tgt;
	tgt->task = p;
	tgt->rq = NULL;
	tgt->quanta = 0;
	INIT_LIST_HEAD(&tgt->list);

	/* Choose the core with the lowest number of READY + RUNNING tasks */
	dst = p->cpu;
	dst_nr = ipanema_state(dst).ready.nr_tasks + (ipanema_state(dst).current_0 ? 1 : 0);
	for_each_cpu(cpu, cstate_info.active_cores) {
		cpu_nr = ipanema_state(cpu).ready.nr_tasks + (ipanema_state(cpu).current_0 ? 1 : 0);
		if (cpu_nr < dst_nr) {
			dst = cpu;
			dst_nr = cpu_nr;
		}
	}

	return dst;
}

static void ipanema_Simple_new_place(struct ipanema_policy *policy,
				     struct process_event *e)
{
	struct Simple_ipa_process * tgt = policy_metadata(e->target);
	int idlecore_10;

	idlecore_10 = task_cpu(e->target);
	ipa_change_queue_and_core(tgt, &ipanema_state(idlecore_10).ready,
				  READY_STATE, &ipanema_core(idlecore_10));
}

static void ipanema_Simple_new_end(struct ipanema_policy *policy,
				   struct process_event *e)
{
	IPA_EMERG_SAFE("[%d] post new on core %d\n",
		       e->target->pid, e->target->cpu);
}

static void ipanema_Simple_detach(struct ipanema_policy *policy,
				  struct process_event *e)
/* need to free the process metadata memory */
{
	struct Simple_ipa_process * tgt = policy_metadata(e->target);

	ipa_change_queue(tgt, NULL, TERMINATED_STATE);
	kfree(tgt);
}

static void ipanema_Simple_tick(struct ipanema_policy *policy,
				struct process_event *e)
{
	struct Simple_ipa_process * tgt = policy_metadata(e->target);

	tgt->quanta = tgt->quanta + 1;
	if ((tgt->quanta % 10) == 0) {
		ipa_change_queue(tgt,
				 &ipanema_state(task_cpu(tgt->task)).ready,
				 READY_TICK_STATE);
	}

}

static void ipanema_Simple_yield(struct ipanema_policy *policy,
				 struct process_event *e)
{
	struct Simple_ipa_process * tgt = policy_metadata(e->target);

	ipa_change_queue(tgt, &ipanema_state(task_cpu(tgt->task)).ready,
			 READY_STATE);
}

static void ipanema_Simple_block(struct ipanema_policy *policy,
				 struct process_event *e)
{
	struct Simple_ipa_process * tgt = policy_metadata(e->target);

	ipa_change_queue(tgt, NULL, BLOCKED_STATE);
}

static int ipanema_Simple_unblock_prepare(struct ipanema_policy *policy,
					  struct process_event *e)
{
	struct task_struct *p = e->target;
	int dst, cpu;
	int cpu_nr, dst_nr;

	dst = p->cpu;
	dst_nr = ipanema_state(dst).ready.nr_tasks + (ipanema_state(dst).current_0 ? 1 : 0);
	for_each_cpu(cpu, cstate_info.active_cores) {
		cpu_nr = ipanema_state(cpu).ready.nr_tasks + (ipanema_state(cpu).current_0 ? 1 : 0);
		if (cpu_nr < dst_nr) {
			dst = cpu;
			dst_nr = cpu_nr;
		}
	}

	return dst;
}

static void ipanema_Simple_unblock_place(struct ipanema_policy *policy,
					 struct process_event *e)
{
	struct Simple_ipa_process * tgt = policy_metadata(e->target);
	int idlecore_11;

	idlecore_11 = task_cpu(e->target);
	ipa_change_queue_and_core(tgt, &ipanema_state(idlecore_11).ready,
				  READY_STATE, &ipanema_core(idlecore_11));
}

static void ipanema_Simple_unblock_end(struct ipanema_policy *policy,
				       struct process_event *e)
{
	IPA_EMERG_SAFE("[%d] post unblock on core %d\n",
		       e->target->pid, e->target->cpu);
}

static void ipanema_Simple_schedule(struct ipanema_policy *policy,
				    unsigned int cpu)
{
	struct task_struct *p;
	struct Simple_ipa_process * _fresh_12;

	p = ipanema_first_task(&ipanema_state(cpu).ready);
	if (!p)
		return;

	_fresh_12 = policy_metadata(p);
	if (_fresh_12) {
		ipa_change_proc(_fresh_12, &ipanema_state(cpu).current_0,
				CURRENT_0_STATE);
	}
}

static void ipanema_Simple_core_entry(struct ipanema_policy *policy,
				      struct core_event *e)
{
	struct Simple_ipa_core * tgt = &ipanema_core(e->target);

	set_active_core(tgt, cstate_info.active_cores);
}

static void ipanema_Simple_core_exit(struct ipanema_policy *policy,
				     struct core_event *e)
{
	struct Simple_ipa_core * tgt = &ipanema_core(e->target);

	set_inactive_core(tgt, cstate_info.active_cores);
	/* TODO: Migrate all tasks to another cpu */
}

static void ipanema_Simple_load_balance(struct ipanema_policy *policy,
					struct core_event *e,
					struct Simple_ipa_sched_domain *sd)
{
	unsigned int cpu = e->target;
	unsigned int victim, busiest = cpu;
	unsigned int nr_victim, nr_cpu, nr_busiest = get_policy_rq(cpu, ready).nr_tasks;
	struct task_struct *p = NULL;
	struct Simple_ipa_process *tgt;
	LIST_HEAD(stolen_tasks);
	unsigned int nr_theft;
	unsigned int nr_dequeued = 0, nr_enqueued = 0;
	unsigned long flags;
	ktime_t old;

	/* Select the busiest core */
	for_each_cpu(victim, sd->cores) {
		nr_victim = get_policy_rq(victim, ready).nr_tasks;
		if (nr_victim > nr_busiest + 1) {
			busiest = victim;
			nr_busiest = nr_victim;
		}
	}
	if (cpu == busiest)
		goto forward_next_balance;

	/*
	 * Lock the busiest core and steal enough tasks to balance cpu and
	 * busiest
	 */
	local_irq_save(flags);
	if (!ipanema_trylock_core(busiest)) {
		local_irq_restore(flags);
		goto forward_next_balance;
	}
	nr_busiest = get_policy_rq(busiest, ready).nr_tasks;
	nr_cpu = get_policy_rq(cpu, ready).nr_tasks;
	nr_theft = (nr_busiest - nr_cpu) / 2;
	while (nr_theft--) {
		p = ipanema_first_task(&get_policy_rq(busiest, ready));
		if (!p)
			continue;
		if (ipanema_task_state(p) != IPANEMA_READY)
			continue;

		tgt = policy_metadata(p);
		list_add(&tgt->list, &stolen_tasks);
		tgt->rq = NULL;
		change_state(p, IPANEMA_MIGRATING, cpu, NULL);
		nr_dequeued++;
	}
	ipanema_unlock_core(busiest);

	/*
	 * Lock cpu and add tasks to READY runqueue
	 */
	ipanema_lock_core(cpu);
	while (!list_empty(&stolen_tasks)) {
		tgt = list_first_entry(&stolen_tasks, struct Simple_ipa_process,
				       list);
		list_del_init(&tgt->list);
		p = tgt->task;
		tgt->rq = &get_policy_rq(cpu, ready);
		change_state(p, IPANEMA_READY, cpu, tgt->rq);
		nr_enqueued++;
	}
	ipanema_unlock_core(cpu);
	local_irq_restore(flags);

forward_next_balance:
	/* Update sd->next_balance */
	old = sd->next_balance;
	sd->next_balance = ktime_add(ktime_get(),
				     ms_to_ktime(10 * cpumask_weight(sd->cores)));
}

/* Called periodically by the runtime */
static void ipanema_Simple_balancing(struct ipanema_policy *policy,
				     struct core_event *e)
{
	ktime_t now = ktime_get();
	struct Simple_ipa_core *c = &ipanema_core(e->target);
	struct Simple_ipa_sched_domain *sd;
	int i;

	for (i = 0; i < c->___sched_domains_idx; i++) {
		sd = &c->sd[i];
		if (ktime_before(sd->next_balance, now)) {
			sd->periodic_count++;
			ipanema_Simple_load_balance(policy, e, sd);
		}
	}
}

/*
 * Called with no lock held.
 */
static void ipanema_Simple_newly_idle(struct ipanema_policy *policy,
				      struct core_event *e)
{
	struct Simple_ipa_core *c = &ipanema_core(e->target);
	struct Simple_ipa_sched_domain *sd;
	int i, cpu = e->target;

	for (i = 0; i < c->___sched_domains_idx; i++) {
		sd = &c->sd[i];
		sd->idle_count++;
		ipanema_Simple_load_balance(policy, e, sd);
		if (ipanema_state(cpu).ready.nr_tasks > 0)
			break;
	}
}

static void ipanema_Simple_enter_idle(struct ipanema_policy *policy,
				      struct core_event *e)
{
	/* set_inactive_core(&ipanema_core(e->target), cstate_info.active_cores, */
	/* 		  INACTIVE_CORES_STATE); */
}

static void ipanema_Simple_exit_idle(struct ipanema_policy *policy,
				     struct core_event *e)
{
	/* set_active_core(&ipanema_core(e->target), cstate_info.active_cores, */
	/* 		ACTIVE_CORES_STATE); */
}

static int ipanema_Simple_init(struct ipanema_policy * policy)
{
	return 0;
}

static bool ipanema_Simple_attach(struct ipanema_policy * policy,
				  struct task_struct * _fresh_13,
				  char * command)
{
	return true;
}

int ipanema_Simple_free_metadata(struct ipanema_policy *policy)
{
	kfree(policy->data);
	return 0;
}

int ipanema_Simple_can_be_default(struct ipanema_policy *policy)
{
	return 1;
}

struct ipanema_module_routines ipanema_Simple_routines =
{
	.order_process	  = ipanema_Simple_order_process,
	.get_core_state   = ipanema_Simple_get_core_state,
	.new_prepare	  = ipanema_Simple_new_prepare,
	.new_place	  = ipanema_Simple_new_place,
	.new_end	  = ipanema_Simple_new_end,
	.tick		  = ipanema_Simple_tick,
	.yield		  = ipanema_Simple_yield,
	.block		  = ipanema_Simple_block,
	.unblock_prepare  = ipanema_Simple_unblock_prepare,
	.unblock_place	  = ipanema_Simple_unblock_place,
	.unblock_end	  = ipanema_Simple_unblock_end,
	.terminate	  = ipanema_Simple_detach,
	.schedule	  = ipanema_Simple_schedule,
	.newly_idle	  = ipanema_Simple_newly_idle,
	.enter_idle	  = ipanema_Simple_enter_idle,
	.exit_idle	  = ipanema_Simple_exit_idle,
	.balancing_select = ipanema_Simple_balancing,
	.core_entry	  = ipanema_Simple_core_entry,
	.core_exit	  = ipanema_Simple_core_exit,
	.init		  = ipanema_Simple_init,
	.free_metadata	  = ipanema_Simple_free_metadata,
	.can_be_default	  = ipanema_Simple_can_be_default,
	.attach		  = ipanema_Simple_attach
};

static int create_scheduling_domains(unsigned int cpu)
{
	struct topology_level *t = per_cpu(topology_levels, cpu);
	struct Simple_ipa_core *c = &ipanema_core(cpu);
	size_t nr_levels = 0, sd_size = sizeof(struct Simple_ipa_sched_domain);
	ktime_t now = ktime_get();
	u64 next_lb;

	/*
	 * If create_scheduling_domains() was already called for this cpu, we
	 * must free the sd field before rebuilding the hierarchy. Should not be
	 * the case
	 */
	if (c->sd)
		kfree(c->sd);

	c->sd = NULL;
	c->___sched_domains_idx = 0;

	/* for each topological level exported by the runtime for cpu */
	while (t) {
		/* expand sd array in core struct by 1 sd */
		c->sd = krealloc(c->sd, (nr_levels + 1) * sd_size, GFP_KERNEL);
		if (!c->sd)
			goto mem_fail;
		/* copy the hw topology cpumask */
		cpumask_copy(c->sd[nr_levels].cores, &t->cores);
		/* copy the hw topology flags */
		c->sd[nr_levels].flags = t->flags;
		/* init sd fields */
		c->sd[nr_levels].___sched_group_idx = 0;
		c->sd[nr_levels].groups = NULL;
		next_lb = cpumask_weight(c->sd[nr_levels].cores);
		c->sd[nr_levels].next_balance = ktime_add(now,
							  ms_to_ktime(next_lb));
		c->sd[nr_levels].idle_count = 0;
		c->sd[nr_levels].periodic_count = 0;
		nr_levels++;
		t = t->next;
	}
	c->___sched_domains_idx = nr_levels;

	return 0;

mem_fail:
	c->___sched_domains_idx = 0;
	kfree(c->sd);
	c->sd = NULL;
	return -ENOMEM;
}

static int build_groups(unsigned int cpu, struct Simple_ipa_sched_domain *sd)
{
	cpumask_t done;
	unsigned int cpu_idx, level = 1, i;
	struct Simple_ipa_core *c = &ipanema_core(cpu);

	/* cleanup current groups if necessary. Should not be the case */
	if (sd->groups)
		kfree(sd->groups);
	sd->groups = NULL;
	sd->___sched_group_idx = 0;

	cpumask_clear(&done);
	/* create the group containing cpu (previous domain) */
	sd->groups = krealloc(sd->groups,
			      sizeof(struct Simple_ipa_sched_group),
			      GFP_KERNEL);
	if (!sd->groups)
		goto mem_fail;
	/* if sd is the lowest domain, just add cpu in the group */
	cpumask_clear(sd->groups[0].cores);
	if (sd == c->sd) {
		cpumask_set_cpu(cpu, sd->groups[0].cores);
		cpumask_set_cpu(cpu, &done);
	} else {
		cpumask_copy(sd->groups[0].cores, sd[-1].cores);
		cpumask_or(&done, &done, sd->groups[0].cores);
	}

	/*
	 * for each cpu in the domain, if it is not already in a group,
	 * build its group and add the cpus in this new group to done.
	 */
	for_each_cpu_wrap(cpu_idx, sd->cores, cpu) {
		/* if already done, continue */
		if (cpumask_test_cpu(cpu_idx, &done))
			continue;

		/* add a group to the domain */
		sd->groups = krealloc(sd->groups,
				      (level + 1) * sizeof(struct Simple_ipa_sched_group),
				      GFP_KERNEL);
		if (!sd->groups)
			goto mem_fail;

		cpumask_clear(sd->groups[level].cores);
		/* search for the lowest domain of cpu_idx containing cpu */
		c = &ipanema_core(cpu_idx);
		for (i = 0; i < c->___sched_domains_idx; i++) {
			if (cpumask_test_cpu(cpu, c->sd[i].cores)) {
				/*
				 * if it's cpu_idx lowest domain, just add
				 * cpu_idx, else, add the previous domain
				 */
				if (i == 0)
					cpumask_set_cpu(cpu_idx,
							sd->groups[level].cores);
				else
					cpumask_andnot(sd->groups[level].cores,
						       c->sd[i - 1].cores,
						       &done);
				break;
			}
		}
		if (i == c->___sched_domains_idx)
			pr_info("%s: from %d point of view, %d is nowhere\n",
				__FUNCTION__, cpu_idx, cpu);
		else
			cpumask_or(&done, &done, sd->groups[level].cores);
		level++;
	}

	sd->___sched_group_idx = level;

	return 0;

mem_fail:
	sd->___sched_group_idx = 0;
	kfree(sd->groups);
	sd->groups = NULL;
	return -ENOMEM;
}

/* Scheduling domains must be up to date for all CPUs */
static int create_scheduling_groups(unsigned int cpu)
{
	struct Simple_ipa_core *c = &ipanema_core(cpu);
	int i, ret = 0;

	/*
	 * for each domain (starting from the bigger one), we must build a list
	 * of groups. These groups are domains from a lower level in the
	 * hierarchy
	 */
	for (i = 0; i < c->___sched_domains_idx; i++) {
		ret = build_groups(cpu, &c->sd[i]);
		if (ret != 0) {
			pr_info("%s: failed on cpu%d, domain%d\n",
				__FUNCTION__, cpu, i);
			break;
		}
	}

	return ret;
}

static void build_hierarchy(void)
{
	int cpu;

	/* Update hierarchy for all cpus handled by the policy */
	for_each_possible_cpu(cpu) {
		create_scheduling_domains(cpu);
	}
	for_each_possible_cpu(cpu) {
		create_scheduling_groups(cpu);
	}
}


static int proc_show(struct seq_file *s, void *p)
{
	long cpu = (long) s->private;
	struct task_struct *pos, *n;
	struct Simple_ipa_process *pr, *curr_proc;
	int load_sum = 0;

	ipanema_lock_core(cpu);
	pr = ipanema_state(cpu).current_0;
	seq_printf(s, "CPU: %ld\n", cpu);
	seq_printf(s, "RUNNING (policy): %d (%d)\n",
	pr ? pr->task->pid : -1,
	pr ? pr->load : -1);
	n = per_cpu(ipanema_current, cpu);
	seq_printf(s, "RUNNING (runtime): %d\n", n ? n->pid : -1);
	load_sum += pr ? pr->load : 0;
	seq_printf(s, "READY: ");
	rbtree_postorder_for_each_entry_safe(pos, n,
					     &(ipanema_state(cpu).ready).root,
					     ipanema_metadata.node_runqueue) {
		curr_proc = (struct Simple_ipa_process *)policy_metadata(pos);
		load_sum += curr_proc->load;
		seq_printf(s, "%d (%d) -> ", pos->pid, curr_proc->load);
	}

	seq_printf(s, "\n");
	seq_printf(s, "COUNT(READY) = %d\n", count(IPANEMA_READY, cpu));
	seq_printf(s, "nr_ready = %d\n", ipanema_state(cpu).ready.nr_tasks);
	seq_printf(s, "load = %d\n", ipanema_core(cpu).cload);
	seq_printf(s, "load_sum = %d\n", load_sum);

	ipanema_unlock_core(cpu);

	return 0;
}

static int proc_open(struct inode *inode, struct file *file)
{
	long cpu;

	kstrtol(file->f_path.dentry->d_iname, 10, &cpu);
	return single_open(file, proc_show, (void *)cpu);
}

static struct file_operations proc_fops = {

	.owner	 = THIS_MODULE,
	.open	 = proc_open,
	.read	 = seq_read,
	.llseek	 = seq_lseek,
	.release = single_release,
};

static int proc_topo_show(struct seq_file *s, void *p)
{
	int cpu, level, sd_lvl;
	struct Simple_ipa_core *c;
	struct Simple_ipa_sched_domain *sd;
	struct Simple_ipa_sched_group *sg;

	for_each_possible_cpu(cpu) {
		seq_printf(s, "cpu%d\n", cpu);
		c = &ipanema_core(cpu);
		for (level = 0; level < c->___sched_domains_idx; level++) {
			sd = &c->sd[level];
			seq_printf(s, "  +--> domain%d: %*pbl  [size=%d] (idle_count=%u, periodic_count=%u)\n",
				   level, cpumask_pr_args(sd->cores),
				   cpumask_weight(sd->cores),
				   sd->idle_count, sd->periodic_count);
			for (sd_lvl = 0; sd_lvl < sd->___sched_group_idx; sd_lvl++) {
				sg = &sd->groups[sd_lvl];
				seq_printf(s, "         +--> group%d: %*pbl\n",
					   sd_lvl, cpumask_pr_args(sg->cores));
			}
		}
	}

	return 0;
}

static int proc_topo_open(struct inode *inode, struct file *file)
{
	return single_open(file, proc_topo_show, NULL);
}

static struct file_operations proc_topo_fops = {
	.owner   = THIS_MODULE,
	.open    = proc_topo_open,
	.read    = seq_read,
	.llseek  = seq_lseek,
	.release = single_release,
};

int init_module(void)
{
	int res, cpu;
	struct proc_dir_entry *procdir = NULL;
	char procbuf[10];

	/* Initialize scheduler variables with non-const value (function call) */
	for_each_possible_cpu(cpu) {
		ipanema_core(cpu).id = cpu;
		/* READY rq */
		ipanema_state(cpu).ready.cpu = cpu;
		ipanema_state(cpu).ready.nr_tasks = 0;
		ipanema_state(cpu).ready.root.rb_node = NULL;
		ipanema_state(cpu).ready.state = get_class(READY_STATE);
	}

	/* allocation of every cpumask_var_t of struct core_state_info */
	if (!zalloc_cpumask_var(&(cstate_info.active_cores), GFP_KERNEL)) {
		res = -ENOMEM;
		goto clean_cpumask_var;
	}

	/* Allocate & setup the ipanema_module */
	module = kmalloc(sizeof(struct ipanema_module), GFP_KERNEL);
	if (!module) {
		res = -ENOMEM;
		goto clean_cpumask_var;
	}

	/* build hierarchy with topology */
	build_hierarchy();

	module->name = name;
	module->routines = &ipanema_Simple_routines;
	module->kmodule = THIS_MODULE;

	/* Register module to the runtime */
	res = ipanema_add_module(module);
	if (res) {
		switch (res) {
		case -ETOOMANYMODULES:
			printk("[IPANEMA] ERROR: too many loaded modules.\n");
			break;
		case -EINVAL:
			printk("[IPANEMA] ERROR: unable to load module. A module with the same name is already loaded.\n");
			break;
		default:
			printk("[IPANEMA] ERROR: couldn't load module.\n");
		}
		goto clean_module;
	}

	/* Create /proc/Simple/<cpu> and topology files */
	procdir = proc_mkdir(name, NULL);
	if (!procdir) {
		pr_info("%s: /proc/%s creation failed\n", name, name);
		goto clean_module;
	}
	for_each_possible_cpu(cpu) {
		scnprintf(procbuf, 10, "%d", cpu);
		if (!proc_create(procbuf, 0444, procdir, &proc_fops))
			pr_info("%s: /proc/%s/%s creation failed\n",
				name, name, procbuf);
	}
	if (!proc_create("topology", 0444, procdir, &proc_topo_fops))
		pr_info("%s: /proc/%s/topology creation failed\n",
			name, name);

	return 0;

 clean_module:
	kfree(module);
 clean_cpumask_var:
	/* deallocation of every cpumask_var_t of struct core_state_info */
	free_cpumask_var(cstate_info.active_cores);

	return res;

}

void cleanup_module(void)
{
	int res;

        remove_proc_subtree(name, NULL);
        
        res = ipanema_remove_module(module);
	if (!res)
		goto end;
	switch (res) {
	case -EMODULENOTFOUND:
		printk ("[IPANEMA] ERROR: module not found... Shouldn't happen !\n");
		/* We can safely exit. */
		break;
	case -EMODULEINUSE:
		printk ("[IPANEMA] ERROR: module in use! Remove all instances from /proc/ipanema_policies. Shouldn't happen\n");
		break;
	default:
		printk ("[IPANEMA] ERROR: unknown error (%d).\n", res);
	}
	return;

end:
        kfree(module);
        /* deallocation of every cpumask_var_t of struct core_state_info */
        free_cpumask_var(cstate_info.active_cores);
}

MODULE_AUTHOR("Ipanema Compiler");
MODULE_DESCRIPTION("Simple scheduling policy");
MODULE_LICENSE("GPL");
