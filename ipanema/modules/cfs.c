/*
  Automatically generated from an Ipanema specification by the Ipanema compiler
  Copyright (C) 2016 INRIA, LIG, EPFL, I3S
  --- Do not edit this file ---
*/

/* checksum = 13410 */
#define LINUX

#include <linux/delay.h>
#include <linux/ipanema.h>
#include <linux/ipanema_rbtree.h>
#include <linux/ktime.h>
#include <linux/lockdep.h>
#include <linux/module.h>
#include <linux/proc_fs.h>
#include <linux/sched.h>
#include <linux/sched/task.h>
#include <linux/seq_file.h>
#include <linux/slab.h>
#include <linux/sort.h>
#include <linux/threads.h>


#define ipanema_assert(x) do{if(!(x)) panic("Error in " #x "\n");} while(0)
#define time_to_ticks(x) ktime_to_ns(x) * HZ / 1000000000
#define ticks_to_time(x) ns_to_ktime(x * 1000000000 / HZ)

static char *name = "cfs";
static struct ipanema_module *module;

static int max_quanta = 100;
#define  CURRENT_0_STATE  1 << 0 /* File "compiler/compiler/compile_misc.ml", line 67, characters 43-50 */
#define  READY_STATE  1 << 1 /* File "compiler/compiler/compile_misc.ml", line 64, characters 43-50 */
#define  BLOCKED_STATE  1 << 2 /* File "compiler/compiler/compile_misc.ml", line 64, characters 43-50 */
#define  TERMINATED_STATE  1 << 3 /* File "compiler/compiler/compile_misc.ml", line 64, characters 43-50 */
#define  READY_TICK_STATE  1 << 4 /* File "compiler/compiler/compile_misc.ml", line 53, characters 68-75 */
#define  MIGRATING_STATE  1 << 5 /* File "compiler/compiler/compile_misc.ml", line 54, characters 71-78 */
#define  ACTIVE_CORES_STATE  1 << 0 /* File "compiler/compiler/compile_misc.ml", line 82, characters 42-49 */
#define  IDLE_CORES_STATE  1 << 1 /* File "compiler/compiler/compile_misc.ml", line 82, characters 42-49 */

struct cfs_ipa_process;
struct cfs_ipa_core;
struct cfs_ipa_sched_domain;
struct cfs_ipa_sched_group;

/* definition of protocol states */
struct state_info
{
	struct cfs_ipa_process *current_0; /* private / unshared */
        struct ipanema_rq ready; /* public / shared */
};


// At least a READY queue is often shared.
// Optimization: use DEFINE_PER_CPU_ALIGNED(type, name) otherwise.
// See include/linux/percpu-defs.h for more information.
DEFINE_PER_CPU_SHARED_ALIGNED(struct state_info, state_info);



/* definition of core's states */
struct core_state_info
{
	cpumask_var_t active_cores;
        cpumask_var_t idle_cores;
};


static struct core_state_info cstate_info;

struct cfs_ipa_process 
{
	/* process attributes
         *  specified by the scheduling policy
         *  in the process = {...} declaration
         */
        int state; // Internal
        struct ipanema_rq * rq; // Internal
        struct rb_node node; // Internal
        struct task_struct * task; // Internal
        ktime_t vruntime;
        ktime_t last_sched;
        int load;
        int curr_quanta;
};

struct cfs_ipa_core 
{
	/* core attributes
         *  specified by the scheduling policy
         *  in the core = {...} declaration
         */
        int state; // Internal
        cpumask_var_t * cpuset; // Internal
        int ___sched_domains_idx; // Internal
        int id; // System
        int cload;
	ktime_t min_vruntime;
        struct cfs_ipa_sched_domain * sd;
};

struct cfs_ipa_sched_group 
{
	/* group attributes
         *  specified by the scheduling policy
         *  in the group = {...} declaration
         */
        cpumask_var_t cores;
        int capacity;
};

struct cfs_ipa_sched_domain 
{
	/* domain attributes
         *  specified by the scheduling policy
         *  in the domain = {...} declaration
         */
        int ___sched_group_idx; // Internal
        struct cfs_ipa_sched_group * groups;
        cpumask_var_t cores;
	int flags; // Internal
	ktime_t next_balance;
	unsigned int count;
};

DEFINE_PER_CPU(struct cfs_ipa_core, core);

/* Function declarations */
static void update_thread (struct cfs_ipa_process *);
static void update_load (struct cfs_ipa_process *, int);

inline static void update_thread(struct cfs_ipa_process *p)
{
	ktime_t now = ktime_get();
	ktime_t delta = ktime_sub(now, p->last_sched);
	int cpu = task_cpu(p->task);
	struct cfs_ipa_core *c = &ipanema_core(cpu);
                
	p->vruntime = ktime_add(p->vruntime, delta);
	if (p->vruntime > c->min_vruntime)
		c->min_vruntime = p->vruntime;

	p->curr_quanta = p->curr_quanta + delta;
	p->last_sched = now;
}

inline static void update_load(struct cfs_ipa_process * p, int quanta)
{
	int load = 1024 - ((1024 * (max_quanta - quanta)) / max_quanta);
        
        p->load = ((8 * p->load) + (2 * load)) / 10;

}

static int ipanema_cfs_order_process(struct ipanema_policy *policy,
                                      struct task_struct *a,
                                      struct task_struct *b)
{
	struct cfs_ipa_process *pa = policy_metadata(a);
	struct cfs_ipa_process *pb = policy_metadata(b);

        return pa->vruntime - pb->vruntime;
}

static int get_class(int state)
{
	switch(state) {
        	case CURRENT_0_STATE: return IPANEMA_RUNNING;
                case READY_STATE: return IPANEMA_READY;
                case BLOCKED_STATE: return IPANEMA_BLOCKED;
                case TERMINATED_STATE: return IPANEMA_TERMINATED;
                case READY_TICK_STATE: return IPANEMA_READY_TICK;
                case MIGRATING_STATE: return IPANEMA_MIGRATING;
                default: return -1;
        }
}

static void ipa_change_proc(struct cfs_ipa_process *proc, struct cfs_ipa_process **dst, int state)
{
	*dst = proc;
	proc->state = state;
	proc->rq = NULL;
	change_state(proc->task, get_class(state), task_cpu(proc->task), NULL);
}

static void ipa_change_queue(struct cfs_ipa_process *proc, struct ipanema_rq *rq, int state)
{
	if(proc->state == CURRENT_0_STATE)
		ipanema_state(task_cpu(proc->task)).current_0 = NULL;
	proc->state = state;
	proc->rq = rq;
	change_state(proc->task, get_class(state), task_cpu(proc->task), rq);
}


static void ipa_change_queue_and_core(struct cfs_ipa_process *proc, struct ipanema_rq *rq, int state, struct cfs_ipa_core *core)
{
	if(proc->state == CURRENT_0_STATE)
		ipanema_state(task_cpu(proc->task)).current_0 = NULL;
	proc->state = state;
	proc->rq = rq;
	change_state(proc->task, get_class(state), core->id, rq);
}

static void set_active_core(struct cfs_ipa_core *core, cpumask_var_t *cores, int state)
{
	core->state = state;
	if (*(core->cpuset))
		cpumask_clear_cpu(core->id, *(core->cpuset));
	cpumask_set_cpu(core->id, *cores);
	core->cpuset = cores;
}

static void set_sleeping_core(struct cfs_ipa_core *core, cpumask_var_t *cores, int state)
{
	core->state = state;
	if (*(core->cpuset))
		cpumask_clear_cpu(core->id, *(core->cpuset));
	cpumask_set_cpu(core->id, *cores);
	core->cpuset = cores;
}

static enum ipanema_core_state get_core_state(int state)
{
	switch (state) {
	case ACTIVE_CORES_STATE:
		return IPANEMA_ACTIVE_CORE;
	case IDLE_CORES_STATE:
		return IPANEMA_IDLE_CORE;
	default:
		return -1;
	}
}

static int ipanema_cfs_get_core_state(struct ipanema_policy *policy,
					 struct core_event *e)
{
	return get_core_state(ipanema_core(e->target).state);
}

static int migrate_from_to(struct cfs_ipa_core *busiest,
			   struct cfs_ipa_core *self_38)
{
	struct task_struct *pos, *n;
        LIST_HEAD(tasks);
        struct ipanema_metadata * imd;
        struct cfs_ipa_process *t;
        int dbg_cpt = 0, ret, self_cload;
	unsigned long flags;
        
        /* Remove tasks from busiest */
	local_irq_save(flags);
	if (!ipanema_trylock_core(busiest->id)) {
		local_irq_restore(flags);
		return -1;
	}

	self_cload = self_38->cload;
        rbtree_postorder_for_each_entry_safe(pos, n,
					     &ipanema_state(busiest->id).ready.root,
					     ipanema_metadata.node_runqueue) {
        	t = policy_metadata(pos);
                if (pos->on_cpu)
                	continue;
                
                if ((busiest->cload - self_cload) >= (2 * t->load)) {
                	list_add(&pos->ipanema_metadata.ipa_tasks, &tasks);
			t->vruntime -= busiest->min_vruntime;
                        ipa_change_queue_and_core(t, NULL, MIGRATING_STATE,
                                                  self_38);
                        dbg_cpt = dbg_cpt + 1;
                        busiest->cload = busiest->cload - t->load;
                        self_cload = self_cload + t->load;
                }
                /* Ensure migration cond. and stop cond. use the same ids ! */
                if (busiest->cload == self_cload) 
                	break;
        }
        ipanema_unlock_core(busiest->id);
        ret = dbg_cpt;
        
        /* Add them to my queue */
        ipanema_lock_core(self_38->id);
        while (!list_empty(&tasks)) {
        	imd = list_first_entry(&tasks, struct ipanema_metadata,
				       ipa_tasks);
                pos = container_of(imd, struct task_struct, ipanema_metadata);
		t = policy_metadata(pos);
		t->vruntime += self_38->min_vruntime;
                ipa_change_queue(t, &ipanema_state(self_38->id).ready,
				 READY_STATE);
		self_38->cload = self_38->cload + t->load;
                list_del_init(&imd->ipa_tasks);
                dbg_cpt--;
                
        }
        /* self_38->load = self_38->load + (my_load - m_load_tmp); */
	ipanema_unlock_core(self_38->id);
	local_irq_restore(flags);
        
        if (dbg_cpt != 0)
        	IPA_EMERG_SAFE("Some tasks (%d) were lost on a migration from %d to %d\n",
        		dbg_cpt, busiest->id, self_38->id);
        
        return ret;
}

static struct cfs_ipa_sched_group *
find_busiest_group(struct ipanema_policy *policy,
		   struct cfs_ipa_sched_domain *sd)
{
	struct cfs_ipa_sched_group *sg = sd->groups, *busiest = NULL;
	unsigned int max_avg_load = 0, avg_load;
	int cpu, i, nr_cpus;

	/* for each group, compute average load, and find max */
	for (i = 0; i < sd->___sched_group_idx; sg++, i++) {
		avg_load = 0;
		nr_cpus = 0;
		for_each_cpu_and(cpu, sd->cores, &policy->allowed_cores) {
			avg_load += (&ipanema_core(cpu))->cload;
			nr_cpus++;
		}
		avg_load = avg_load / nr_cpus;
		if (avg_load > max_avg_load) {
			max_avg_load = avg_load;
			busiest = sg;
		}
	}

	return busiest;
}

static struct cfs_ipa_core *
find_busiest_cpu_group(struct ipanema_policy *policy,
			struct cfs_ipa_sched_group *sg)
{
	int cpu;
	unsigned int max_load = 0;
	struct cfs_ipa_core *c = NULL, *busiest = NULL;

	for_each_cpu_and(cpu, sg->cores, &policy->allowed_cores) {
		c = &ipanema_core(cpu);
		if (c->cload > max_load) {
			max_load = c->cload;
			busiest = c;
		}
	}

	return busiest;
}

static void steal_for_dom(struct ipanema_policy *policy,
			  struct cfs_ipa_core *core_31,
			  struct cfs_ipa_sched_domain *sd)
{
        struct cfs_ipa_core *selected;
	struct cfs_ipa_sched_group *sg;

	/*
	 * find the busiest group of this domain
	 *     --> phase 1 of load balancing (filter)
	 */
	sg = find_busiest_group(policy, sd);
	if (!sg)
		goto forward_next_balance;

	/* if busiest group contains current cpu, abort */
	if (cpumask_test_cpu(core_31->id, sg->cores))
		goto forward_next_balance;

	/*
	 * select busiest cpu in busiest group
	 *     --> phase 2 of load balancing (select)
	 */
	selected = find_busiest_cpu_group(policy, sg);
	if (!selected)
		goto forward_next_balance;

	/*
	 * do actual balancing
	 *     --> phase 3 of load balancing (steal)
	 */
	migrate_from_to(selected, core_31);

forward_next_balance:
	sd->count++;
	sd->next_balance = ktime_add(ktime_get(),
				     ms_to_ktime(cpumask_weight(sd->cores)));
}

static void steal_for(struct ipanema_policy *policy, struct cfs_ipa_core *c)
{
	ktime_t now = ktime_get();
	struct cfs_ipa_sched_domain *sd;
	int lvl;

	for (lvl = 0; lvl < c->___sched_domains_idx; lvl++) {
		sd = &c->sd[lvl];
		if (ktime_before(sd->next_balance, now))
			steal_for_dom(policy, c, sd);
	}
}

static struct cfs_ipa_core *
find_idlest_cpu_group(struct ipanema_policy *policy,
		      struct cfs_ipa_sched_group *sg)
{
	int cpu;
	unsigned int min_load = UINT_MAX;
	struct cfs_ipa_core *c = NULL, *idlest = NULL;

	for_each_cpu_and(cpu, sg->cores, &policy->allowed_cores) {
		c = &ipanema_core(cpu);
		/* if cpu is idle, choose it immediately */
		if (cpumask_test_cpu(cpu, cstate_info.idle_cores)) {
			idlest = c;
			goto end;
		}
		/* else, continue to search for idlest cpu */
		if (c->cload < min_load) {
			idlest = c;
			min_load = c->cload;
		}
	}

end:
	return idlest;
}

static struct cfs_ipa_sched_group *
find_idlest_group(struct ipanema_policy *policy,
		  struct cfs_ipa_sched_domain *sd)
{
	struct cfs_ipa_sched_group *sg = sd->groups, *idlest = NULL;
	int i, cpu, nr_cpus;
	unsigned int min_avg_load = UINT_MAX;
	unsigned int avg_load;

	/* for each group, compute average load, and find min */
	for (i = 0; i < sd->___sched_group_idx; sg++, i++) {
		avg_load = 0;
		nr_cpus = 0;
		for_each_cpu_and(cpu, sg->cores, &policy->allowed_cores) {
			avg_load += (&ipanema_core(cpu))->cload;
			nr_cpus++;
		}
		avg_load = avg_load / nr_cpus;
		if (avg_load < min_avg_load) {
			min_avg_load = avg_load;
			idlest = sg;
		}
	}

	return idlest;
}

static int ipanema_cfs_new_prepare(struct ipanema_policy *policy,
                                    struct process_event *e)
{
	struct cfs_ipa_process *tgt;
	struct cfs_ipa_sched_domain *sd;
	struct cfs_ipa_sched_group *sg;
	struct cfs_ipa_core *c, *idlest = NULL;
        struct task_struct *task_15;
        
        task_15 = e->target;
        tgt = kzalloc(sizeof(struct cfs_ipa_process), GFP_ATOMIC);
        if (!tgt) 
        	return -1;

        policy_metadata(task_15) = tgt;
        tgt->task = task_15;
        tgt->rq = NULL;

	/* find idlest group in highest domain, then idlest core in this group */
	c = &ipanema_core(task_cpu(task_15));
	sd = c->sd + c->___sched_domains_idx - 1;
	sg = find_idlest_group(policy, sd);
	idlest = find_idlest_cpu_group(policy, sg);

	/* should never happen ? */
	if (!idlest)
		idlest = c;
	tgt->vruntime = idlest->min_vruntime;
	tgt->load = 1024;
	tgt->curr_quanta = 0;
	return idlest->id;
}

static void ipanema_cfs_new_place(struct ipanema_policy *policy,
                                   struct process_event *e)
{
	struct cfs_ipa_process * tgt = policy_metadata(e->target);
        int idlecore_10 = task_cpu(e->target);
	struct cfs_ipa_core *c = &ipanema_core(idlecore_10);

	c->cload += tgt->load;
	smp_wmb();
        ipa_change_queue_and_core(tgt,
                                  &ipanema_state(task_cpu(tgt->task)).ready,
                                  READY_STATE, c);
}

static void ipanema_cfs_new_end(struct ipanema_policy *policy,
                                 struct process_event *e)
{
	IPA_EMERG_SAFE("[%d] post new on core %d\n", e->target->pid, e->target->cpu);
}

static void ipanema_cfs_detach(struct ipanema_policy *policy,
			       struct process_event *e)
/* need to free the process metadata memory */
{
	struct cfs_ipa_process * tgt = policy_metadata(e->target);
	struct cfs_ipa_core *c = &ipanema_core(task_cpu(tgt->task));

        ipa_change_queue(tgt, NULL, TERMINATED_STATE);
	smp_wmb();
	c->cload -= tgt->load;
        kfree(tgt);
}

static void ipanema_cfs_tick(struct ipanema_policy *policy,
			     struct process_event *e)
{
	struct cfs_ipa_process * tgt = policy_metadata(e->target);
        
        update_thread((struct cfs_ipa_process *)tgt);
        if (tgt->curr_quanta >= max_quanta)
                ipa_change_queue(tgt,
                                 &ipanema_state(task_cpu(tgt->task)).ready,
                                 READY_TICK_STATE);
}

static void ipanema_cfs_yield(struct ipanema_policy *policy,
			      struct process_event *e)
{
	struct cfs_ipa_process * tgt = policy_metadata(e->target);
	struct cfs_ipa_core *c = &ipanema_core(task_cpu(e->target));
	int old_load = tgt->load;

        update_thread((struct cfs_ipa_process *)tgt);
        update_load((struct cfs_ipa_process *)tgt, tgt->curr_quanta);
        ipa_change_queue(tgt, &ipanema_state(task_cpu(tgt->task)).ready,
                         READY_STATE);
	c->cload += (tgt->load - old_load);
}

static void ipanema_cfs_block(struct ipanema_policy *policy,
			      struct process_event *e)
{
	struct cfs_ipa_process * tgt = policy_metadata(e->target);
	struct cfs_ipa_core *c = &ipanema_core(task_cpu(e->target));
	int old_load = tgt->load;

        update_thread((struct cfs_ipa_process *)tgt);
        update_load((struct cfs_ipa_process *)tgt, tgt->curr_quanta);
        ipa_change_queue(tgt, NULL, BLOCKED_STATE);
	smp_wmb();
	c->cload -= old_load;
}

static struct cfs_ipa_core *find_idle_cpu(struct ipanema_policy *policy,
					  struct cfs_ipa_sched_domain *sd)
{
	int cpu;

	for_each_cpu_and(cpu, sd->cores, &policy->allowed_cores) {
		if (cpumask_test_cpu(cpu, cstate_info.idle_cores))
			return &ipanema_core(cpu);
	}

	return NULL;
}

static int ipanema_cfs_unblock_prepare(struct ipanema_policy *policy,
				       struct process_event *e)
{
	struct task_struct *task_15 = e->target;
	struct cfs_ipa_process *p = policy_metadata(task_15);
	struct cfs_ipa_sched_domain *sd = NULL;
	struct cfs_ipa_sched_group *sg = NULL;
        struct cfs_ipa_core *c, *idlest = NULL;
	int i, flags = 0;

	/* remove min_vruntime from previous cpu */
	c = &ipanema_core(task_cpu(task_15));
	p->vruntime -= c->min_vruntime;

	/* domains where fork placement is allowed */
	flags |= DOMAIN_SMT | DOMAIN_CACHE;

	/* find closest idle core sharing cache */
	for (i = 0; i < c->___sched_domains_idx; i++) {
		sd = c->sd + i;
		if (sd->flags & flags) {
			idlest = find_idle_cpu(policy, sd);
			if (idlest)
				goto end;
		}
	}

	/* no core sharing cache is idle, use closest idlest core */
	for (i = 0; i < c->___sched_domains_idx; i++) {
		sd = c->sd + i;
		if (sd->flags & flags) {
			sg = find_idlest_group(policy, sd);
			idlest = find_idlest_cpu_group(policy, sg);
			if (idlest)
				goto end;
		}
	}

	/* if no core found, wake up on previous core */
	if (!idlest)
		idlest = c;

	/* add min_vruntime from new cpu */
	p->vruntime += idlest->min_vruntime;

end:
        return idlest->id;
}

static void ipanema_cfs_unblock_place(struct ipanema_policy *policy,
                                       struct process_event *e)
{
	struct cfs_ipa_process * tgt = policy_metadata(e->target);
        int idlecore_11 = task_cpu(e->target);
	struct cfs_ipa_core *c = &ipanema_core(idlecore_11);

        c->cload += tgt->load;
	smp_wmb();
        ipa_change_queue_and_core(tgt, &ipanema_state(idlecore_11).ready,
                                  READY_STATE, &per_cpu(core, idlecore_11));
}

static void ipanema_cfs_unblock_end(struct ipanema_policy *policy,
                                     struct process_event *e)
{
	IPA_EMERG_SAFE("[%d] post unblock on core %d\n", e->target->pid,
                e->target->cpu);
}

static void ipanema_cfs_schedule(struct ipanema_policy *policy,
                                  unsigned int cpu)
{
	struct task_struct * task_20 = ipanema_first_task(&ipanema_state(cpu).ready);
        struct cfs_ipa_process * p;
	struct cfs_ipa_core *c;
	int old_load;
        
        if (!task_20) 
        	return;
        
        p = policy_metadata(task_20);
	c = &ipanema_core(task_cpu(task_20));
        {
        	struct timespec ts_12;
                
                getnstimeofday(&ts_12);
                p->last_sched = (ktime_t)timespec_to_ktime(ts_12);
        }
	old_load = p->load;
        p->curr_quanta = 0;
        p->load = 1024;
        if (!p) 
        	return;
        
        ipa_change_proc(p, &ipanema_state(cpu).current_0, CURRENT_0_STATE);
	c->cload += (p->load - old_load);
        return;
}

static void ipanema_cfs_core_entry(struct ipanema_policy *policy,
                                    struct core_event *e)
{
	struct cfs_ipa_core * tgt = &per_cpu(core, e->target);

        set_active_core(tgt, &cstate_info.active_cores, ACTIVE_CORES_STATE);
}

static void ipanema_cfs_core_exit(struct ipanema_policy *policy,
                                   struct core_event *e)
{
	struct cfs_ipa_core *tgt = &per_cpu(core, e->target);

        set_sleeping_core(tgt, &cstate_info.idle_cores, IDLE_CORES_STATE);
}

static void ipanema_cfs_newly_idle(struct ipanema_policy *policy,
                                    struct core_event *e)
{
	struct cfs_ipa_core *c = &per_cpu(core, e->target);
	struct cfs_ipa_sched_domain *sd;
	int lvl;

	for (lvl = 0; lvl < c->___sched_domains_idx; lvl++) {
		sd = &c->sd[lvl];
		steal_for_dom(policy, c, sd);
		if (ipanema_state(c->id).ready.nr_tasks)
			break;
	}
}

static void ipanema_cfs_enter_idle(struct ipanema_policy *policy,
				   struct core_event *e)
{
	struct cfs_ipa_core * tgt = &per_cpu(core, e->target);
        set_sleeping_core(tgt, &cstate_info.idle_cores, IDLE_CORES_STATE);
}

static void ipanema_cfs_exit_idle(struct ipanema_policy *policy,
				  struct core_event *e)
{
	struct cfs_ipa_core * tgt = &per_cpu(core, e->target);
        set_active_core(tgt, &cstate_info.active_cores, ACTIVE_CORES_STATE);
}

static void ipanema_cfs_balancing(struct ipanema_policy *policy,
				  struct core_event *e)
{
	struct cfs_ipa_core * tgt = &per_cpu(core, e->target);
        
        steal_for(policy, tgt);
        return;
}

static int ipanema_cfs_init(struct ipanema_policy * policy)
{
	return 0;
}

static bool ipanema_cfs_attach(struct ipanema_policy * policy,
                                struct task_struct * _fresh_14, char * command)
{
	return true;
}

int ipanema_cfs_free_metadata(struct ipanema_policy *policy)
{
	kfree(policy->data);
        return 0;
}

int ipanema_cfs_can_be_default(struct ipanema_policy *policy)
{
	return 1;
}

struct ipanema_module_routines ipanema_cfs_routines =
{
	.order_process  = ipanema_cfs_order_process,
	.get_core_state = ipanema_cfs_get_core_state,
        .new_prepare
                 = ipanema_cfs_new_prepare,
        .new_place
                 = ipanema_cfs_new_place,
        .new_end = ipanema_cfs_new_end,
        .tick    = ipanema_cfs_tick,
        .yield   = ipanema_cfs_yield,
        .block   = ipanema_cfs_block,
        .unblock_prepare
                 = ipanema_cfs_unblock_prepare,
        .unblock_place
                 = ipanema_cfs_unblock_place,
        .unblock_end
                 = ipanema_cfs_unblock_end,
        .terminate
                 = ipanema_cfs_detach,
        .schedule= ipanema_cfs_schedule,
        .newly_idle
                 = ipanema_cfs_newly_idle,
        .enter_idle
                 = ipanema_cfs_enter_idle,
        .exit_idle
                 = ipanema_cfs_exit_idle,
        .balancing_select
                 = ipanema_cfs_balancing,
        .core_entry
                 = ipanema_cfs_core_entry,
        .core_exit
                 = ipanema_cfs_core_exit,
        .init    = ipanema_cfs_init,
        .free_metadata
                 = ipanema_cfs_free_metadata,
        .can_be_default
                 = ipanema_cfs_can_be_default,
        .attach  = ipanema_cfs_attach
};

static int create_scheduling_domains(unsigned int cpu)
{
	struct topology_level *t = per_cpu(topology_levels, cpu);
	struct cfs_ipa_core *c = &ipanema_core(cpu);
	size_t nr_levels = 0, sd_size = sizeof(struct cfs_ipa_sched_domain);
	ktime_t now = ktime_get();
	u64 next_lb;

	/*
	 * If create_scheduling_domains() was already called for this cpu, we
	 * must free the sd field before rebuilding the hierarchy
	 */
	if (c->sd)
		kfree(c->sd);

	c->sd = NULL;
	c->___sched_domains_idx = 0;

	/* for each topological level exported by the runtime for cpu */
	while (t) {
		/* expand sd array in core struct by 1 sd */
		c->sd = krealloc(c->sd, (nr_levels + 1) * sd_size, GFP_KERNEL);
		if (!c->sd)
			goto mem_fail;
		/* copy the hw topology cpumask */
		cpumask_copy(c->sd[nr_levels].cores, &t->cores);
		/* copy the hw topology flags */
		c->sd[nr_levels].flags = t->flags;
		/* init sd fields */
		c->sd[nr_levels].___sched_group_idx = 0;
		c->sd[nr_levels].groups = NULL;
		next_lb = cpumask_weight(c->sd[nr_levels].cores);
		c->sd[nr_levels].next_balance = ktime_add(now,
							  ms_to_ktime(next_lb));
		c->sd[nr_levels].count = 0;
		nr_levels++;
		t = t->next;
	}
	c->___sched_domains_idx = nr_levels;

	return 0;

mem_fail:
	c->___sched_domains_idx = 0;
	kfree(c->sd);
	c->sd = NULL;
	return -ENOMEM;
}

static int build_groups(unsigned int cpu, struct cfs_ipa_sched_domain *sd)
{
	cpumask_t done;
	unsigned int cpu_idx, level = 1, i;
	struct cfs_ipa_core *c = &ipanema_core(cpu);

	/* cleanup current groups if necessary */
	if (sd->groups)
		kfree(sd->groups);
	sd->groups = NULL;
	sd->___sched_group_idx = 0;

	cpumask_clear(&done);
	/* create the group containing cpu (previous domain) */
	sd->groups = krealloc(sd->groups,
			      sizeof(struct cfs_ipa_sched_group),
			      GFP_KERNEL);
	if (!sd->groups)
		goto mem_fail;
	/* if sd is the lowest domain, just add cpu in the group */
	cpumask_clear(sd->groups[0].cores);
	if (sd == c->sd) {
		cpumask_set_cpu(cpu, sd->groups[0].cores);
		cpumask_set_cpu(cpu, &done);
	} else {
		cpumask_copy(sd->groups[0].cores, sd[-1].cores);
		cpumask_or(&done, &done, sd->groups[0].cores);
	}

	/*
	 * for each cpu in the domain, if it is not already in a group,
	 * build its group and add the cpus in this new group to done.
	 */
	for_each_cpu_wrap(cpu_idx, sd->cores, cpu) {
		/* if already done, continue */
		if (cpumask_test_cpu(cpu_idx, &done))
			continue;

		/* add a group to the domain */
		sd->groups = krealloc(sd->groups,
				      (level + 1) * sizeof(struct cfs_ipa_sched_group),
				      GFP_KERNEL);
		if (!sd->groups)
			goto mem_fail;
		
		cpumask_clear(sd->groups[level].cores);
		/* search for the lowest domain of cpu_idx containing cpu */
		c = &ipanema_core(cpu_idx);
		for (i = 0; i < c->___sched_domains_idx; i++) {
			if (cpumask_test_cpu(cpu, c->sd[i].cores)) {
				/*
				 * if it's cpu_idx lowest domain, just add
				 * cpu_idx, else, add the previous domain cpus
				 * already seen (prevent overlap)
				 */
				if (i == 0)
					cpumask_set_cpu(cpu_idx,
							sd->groups[level].cores);
				else
					cpumask_andnot(sd->groups[level].cores,
						       c->sd[i - 1].cores,
						       &done);
				break;
			}
		}
		if (i == c->___sched_domains_idx)
			pr_info("%s: from %d point of view, %d is nowhere\n",
				__FUNCTION__, cpu_idx, cpu);
		else
			cpumask_or(&done, &done, sd->groups[level].cores);
		level++;
	}

	sd->___sched_group_idx = level;

	return 0;

mem_fail:
	sd->___sched_group_idx = 0;
	kfree(sd->groups);
	sd->groups = NULL;
	return -ENOMEM;
}

/* Scheduling domains must be up to date for all CPUs */
static int create_scheduling_groups(unsigned int cpu)
{
	struct cfs_ipa_core *c = &ipanema_core(cpu);
	int i, ret = 0;

	/*
	 * for each domain (starting from the bigger one), we must build a list
	 * of groups. These groups are domains from a lower level in the
	 * hierarchy
	 */
	for (i = 0; i < c->___sched_domains_idx; i++) {
		ret = build_groups(cpu, &c->sd[i]);
		if (ret != 0) {
			pr_info("%s: failed on cpu%d, domain%d\n",
				__FUNCTION__, cpu, i);
			break;
		}
	}

	return ret;
}

static void build_hierarchy(void)
{
	int cpu;

	/* Update hierarchy for all cpus handled by the policy */
	for_each_possible_cpu(cpu) {
		create_scheduling_domains(cpu);
	}
	for_each_possible_cpu(cpu) {
		create_scheduling_groups(cpu);
	}
}

static int proc_show(struct seq_file *s, void *p)
{
	long cpu = (long) s->private;
        struct task_struct *pos, *n;
        struct cfs_ipa_process *pr, *curr_proc;
        int load_sum = 0;
        
        ipanema_lock_core(cpu);
        pr = ipanema_state(cpu).current_0;
        seq_printf(s, "CPU: %ld\n", cpu);
        seq_printf(s, "RUNNING (policy): %d (%d)\n",
		   pr ? pr->task->pid : -1,
		   pr ? pr->load : -1);
        n = per_cpu(ipanema_current, cpu);
        seq_printf(s, "RUNNING (runtime): %d\n", n ? n->pid : -1);
        load_sum += pr ? pr->load : 0;
        seq_printf(s, "READY: ");
        rbtree_postorder_for_each_entry_safe(pos, n,
					     &(ipanema_state(cpu).ready).root,
					     ipanema_metadata.node_runqueue) {
        	curr_proc = (struct cfs_ipa_process *)policy_metadata(pos);
        	load_sum += curr_proc->load;
        	seq_printf(s, "%d (%d) -> ", pos->pid, curr_proc->load);
        }
        
        seq_printf(s, "\n");
        seq_printf(s, "COUNT(READY) = %d\n", count(IPANEMA_READY, cpu));
        seq_printf(s, "load = %d\n", ipanema_core(cpu).cload);
        seq_printf(s, "load_sum = %d\n", load_sum);
        
        ipanema_unlock_core(cpu);
        
        return 0;
}

static int proc_open(struct inode *inode, struct file *file)
{
	long cpu;

        kstrtol(file->f_path.dentry->d_iname, 10, &cpu);
        return single_open(file, proc_show, (void *)cpu);
}

static struct file_operations proc_fops = {
	
        .owner   = THIS_MODULE,
        .open    = proc_open,
        .read    = seq_read,
        .llseek  = seq_lseek,
        .release = single_release
};

static int proc_topo_show(struct seq_file *s, void *p)
{
	int cpu, level, sd_lvl;
	struct cfs_ipa_core *c;
	struct cfs_ipa_sched_domain *sd;
	struct cfs_ipa_sched_group *sg;

	for_each_possible_cpu(cpu) {
		seq_printf(s, "cpu%d\n", cpu);
		c = &ipanema_core(cpu);
		for (level = 0; level < c->___sched_domains_idx; level++) {
			sd = &c->sd[level];
			seq_printf(s, "  +--> domain%d: %*pbl  [size=%d] (count=%u)\n",
				   level, cpumask_pr_args(sd->cores),
				   cpumask_weight(sd->cores),
				   sd->count);
			for (sd_lvl = 0; sd_lvl < sd->___sched_group_idx; sd_lvl++) {
				sg = &sd->groups[sd_lvl];
				seq_printf(s, "         +--> group%d: %*pbl\n",
					   sd_lvl, cpumask_pr_args(sg->cores));
			}
		}
	}

	return 0;
}

static int proc_topo_open(struct inode *inode, struct file *file)
{
	return single_open(file, proc_topo_show, NULL);
}

static struct file_operations proc_topo_fops = {
	.owner   = THIS_MODULE,
	.open    = proc_topo_open,
	.read    = seq_read,
	.llseek  = seq_lseek,
	.release = single_release,
};

int init_module(void)
{
	int res, cpu;
        struct proc_dir_entry *procdir = NULL;
        char procbuf[10];
        
        /* Initialize scheduler variables with non-const value (function call) */
        for_each_possible_cpu(cpu) {
        	ipanema_core(cpu).id = cpu;
                /* FIXME init of core variables of the user */
		ipanema_core(cpu).cload = 0;
                /* allocation of ipanema rqs */
                ipanema_state(cpu).ready.cpu = cpu;
                ipanema_state(cpu).ready.nr_tasks = 0;
                ipanema_state(cpu).ready.root.rb_node = NULL;
                ipanema_state(cpu).ready.state = IPANEMA_READY;
        }
        
        /* allocation of every cpumask_var_t of struct core_state_info */
        if (!zalloc_cpumask_var(&(cstate_info.active_cores), GFP_KERNEL)) {
        	res = -ENOMEM;
                goto clean_cpumask_var;
        }
        
        if (!zalloc_cpumask_var(&(cstate_info.idle_cores), GFP_KERNEL)) {
        	res = -ENOMEM;
                goto clean_cpumask_var;
        }

	/* build hierarchy with topology */
	build_hierarchy();

	/* Allocate & setup the ipanema_module */
        module = kmalloc(sizeof(struct ipanema_module), GFP_KERNEL);
        if (!module) {
        	res = -ENOMEM;
                goto clean_cpumask_var;
        }
        module->name = name;
        module->routines = &ipanema_cfs_routines;
        module->kmodule = THIS_MODULE;

	/* Register module to the runtime */
	res = ipanema_add_module(module);
	if (res) {
		switch (res) {
		case -ETOOMANYMODULES:
			printk("[IPANEMA] ERROR: too many loaded modules.\n");
			break;
		case -EINVAL:
			printk("[IPANEMA] ERROR: unable to load module. A module with the same name is already loaded.\n");
			break;
		default:
			printk("[IPANEMA] ERROR: couldn't load module.\n");
                }
                goto clean_module;
        }
        
        /*
	 * Create /proc/cfs/<cpu> files and /proc/cfs/topology file
	 * If file creation fails, module insertion does not
	 */
	procdir = proc_mkdir(name, NULL);
	if (!procdir)
		pr_err("%s: /proc/%s creation failed\n", name, name);
	for_each_possible_cpu(cpu) {
		scnprintf(procbuf, 10, "%d", cpu);
		if (!proc_create(procbuf, 0444, procdir, &proc_fops))
			pr_err("%s: /proc/%s/%s creation failed\n",
			       name, name, procbuf);
	}
	if (!proc_create("topology", 0444, procdir, &proc_topo_fops))
		pr_err("%s: /proc/%s/topology creation failed\n",
		       name, name);

        return 0;

clean_module:
        kfree(module);

clean_cpumask_var:
        /* free every cpumask_var_t of struct core_state_info */
        free_cpumask_var(cstate_info.idle_cores);
        free_cpumask_var(cstate_info.active_cores);
        return res;
}

void cleanup_module(void)
{
	int res;
        
        remove_proc_subtree(name, NULL);
        
        res = ipanema_remove_module(module);
	if (!res)
		goto end;
	switch (res) {
	case -EMODULENOTFOUND:
		printk("[IPANEMA] ERROR: module not found... Shouldn't happen !\n");
		/* We can safely exit. */
		break;
	case -EMODULEINUSE:
		printk("[IPANEMA] ERROR: module in use! Remove all instances from /proc/ipanema_policies. Shouldn't happen\n");
		break;
	default:
		printk("[IPANEMA] ERROR: unknown error (%d).\n", res);
        }
        return;
 end:
        kfree(module);
        /* deallocation of every cpumask_var_t of struct core_state_info */
        free_cpumask_var(cstate_info.idle_cores);
        free_cpumask_var(cstate_info.active_cores);
}

MODULE_AUTHOR("Ipanema Compiler");
MODULE_DESCRIPTION("cfs scheduling policy");
MODULE_LICENSE("GPL");
